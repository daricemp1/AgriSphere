# -*- coding: utf-8 -*-
"""test_forecast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UD040k8XBgEHKV1LLUdXWtQlb8IPc60T
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from google.colab import files


uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files
import os, sys

!mkdir -p /content/myproj/utils
# %cd /content/myproj
uploaded = files.upload()  
# If it landed at /content/myproj/forecast.py, move it:
if os.path.exists("forecast.py"):
    os.makedirs("utils", exist_ok=True)
    os.replace("forecast.py", "utils/forecast.py")

sys.path.append("/content/myproj")

!git clone https://github.com/yuqinie98/PatchTST.git /content/PatchTST

!ls /content/PatchTST

pip install NeuralForecast

pip install darts

# eval_forecast_patchtst.py
# Run: python eval_forecast_patchtst.py
import math
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from forecast import fetch_weather_data
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import DistributionLoss
import torch

# Reproducibility
np.random.seed(42)
torch.manual_seed(42)


# Config
COUNTRIES = {
    "Singapore": "singapore",
    "United States": "america",
    "India": "india",
}

PARAMS = ["precipitation", "temp_max", "temp_min", "windspeed", "soil_moisture"]
# forecast horizons
HORIZONS = [3, 7, 14] 
# fetch >2y so a 730-day train window fits + horizon        
HISTORY_YEARS = 3.5
#  use full 2 years at each step            
TRAIN_WINDOW_DAYS = 730       
PATCHTST_CONFIG = {
    "input_size": 365,         
    "h": 14,                   
    "patch_len": 30,
    "stride": 15,
    "revin": True,
    "hidden_size": 64,
    "n_heads": 4,
    "scaler_type": "robust",
    "loss": DistributionLoss(distribution='StudentT', level=[80, 90]),
    "learning_rate": 5e-4,
    "max_steps": 500,
    "val_check_steps": 50,
    "early_stop_patience_steps": 5,  
}

-
# Metric helpers (NumPy arrays)
def safe_mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    try:
        mask = np.abs(y_true) > 1e-9
        return float(np.mean(np.abs((y_pred[mask] - y_true[mask]) / y_true[mask])) * 100.0) if np.any(mask) else float("nan")
    except Exception:
        return float("nan")

def safe_r2(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    try:
        ss_res = float(np.sum((y_true - y_pred) ** 2))
        ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))
        return float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float("nan")
    except Exception:
        return float("nan")

# Backtesting helpers
def _choose_val_size(train_len: int, input_size: int, horizon_days: int) -> int:
    desired = max(14, horizon_days)
    # how many points remain after reserving input_size + 10 buffer?
    slack = train_len - (input_size + 10)
    if slack <= 0:
        return 0
    return max(1, min(desired, slack))

def _fit_patchtst_single_series(train_df: pd.DataFrame, horizon_days: int, val_size: int) -> NeuralForecast:
    """Fit PatchTST via NeuralForecast on a single univariate series (panel format)."""
    config = PATCHTST_CONFIG.copy()
    config["h"] = horizon_days
    model = PatchTST(**config)
    # Reduce Lightning noise a bit
    nf = NeuralForecast(models=[model], freq='D')
    nf.fit(df=train_df, val_size=val_size)   
    return nf

def backtest_patchtst(df: pd.DataFrame, column: str, horizon_days: int, train_window_days: int) -> Dict[str, float]:
    """Backtest PatchTST on one series with a fixed rolling 730-day training window."""
    series_df = df.dropna(subset=[column]).sort_values("ds").reset_index(drop=True)
    if series_df.empty:
        return {"MAE": float("nan"), "RMSE": float("nan"), "MAPE%": float("nan"), "R2": float("nan"), "n_tests": 0}

    # Ensure dense daily index and fill gaps
    series_df['ds'] = pd.to_datetime(series_df['ds'])
    series_df = series_df.set_index('ds').sort_index()
    full_date_range = pd.date_range(start=series_df.index.min(), end=series_df.index.max(), freq='D')
    series_df = series_df.reindex(full_date_range)
    series_df[column] = series_df[column].ffill().bfill()

    # Panel DF for NeuralForecast (unique_id, ds, y)
    panel_df = series_df.reset_index().rename(columns={'index': 'ds', column: 'y'})
    panel_df['unique_id'] = 'series1'

    #  Correct min-length requirement
    min_hist_needed = max(train_window_days, PATCHTST_CONFIG["input_size"])
    if len(panel_df) < (min_hist_needed + horizon_days + 5):
        return {"MAE": float("nan"), "RMSE": float("nan"), "MAPE%": float("nan"), "R2": float("nan"), "n_tests": 0}

    # Determine cutoffs
    min_cutoff = min_hist_needed
    max_cutoff = len(panel_df) - horizon_days
    if max_cutoff - min_cutoff <= 0:
        return {"MAE": float("nan"), "RMSE": float("nan"), "MAPE%": float("nan"), "R2": float("nan"), "n_tests": 0}

    step_size = max(5, (max_cutoff - min_cutoff) // 20)

    errors = []
    for cutoff in range(min_cutoff, max_cutoff, step_size):
        try:
            train_df = panel_df.iloc[:cutoff].copy()
            test_df  = panel_df.iloc[cutoff:cutoff + horizon_days].copy()

            #  Use a fixed rolling window of 730 days for training
            if len(train_df) > train_window_days:
                train_df = train_df.iloc[-train_window_days:].copy()

            # Compute a safe validation size for early stopping
            val_size = _choose_val_size(train_len=len(train_df),
                                        input_size=PATCHTST_CONFIG["input_size"],
                                        horizon_days=horizon_days)

            # Guard: ensure at least input_size + val_size + small buffer
            if len(train_df) < PATCHTST_CONFIG["input_size"] + val_size + 10:
                continue

            nf = _fit_patchtst_single_series(train_df, horizon_days, val_size)
            preds = nf.predict(futr_df=test_df)

            # If distribution output, the median forecast column is "PatchTST-median"; if point, it's "PatchTST"
            if 'PatchTST' in preds.columns:
                y_pred = preds['PatchTST'].values
            elif 'PatchTST-median' in preds.columns:
                y_pred = preds['PatchTST-median'].values
            else:
                y_cols = [c for c in preds.columns if c not in ('unique_id', 'ds')]
                if not y_cols:
                    continue
                y_pred = preds[y_cols[0]].values

            y_true = test_df['y'].values[:len(y_pred)]
            y_pred = y_pred[:len(y_true)]

            mae_val  = float(np.mean(np.abs(y_true - y_pred)))
            rmse_val = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))
            mape_val = safe_mape(y_true, y_pred)
            r2_val   = safe_r2(y_true, y_pred)

            if not (np.isfinite(mae_val) and np.isfinite(rmse_val)):
                continue

            errors.append((mae_val, rmse_val, mape_val, r2_val))

        except Exception as e:
            if len(errors) == 0:
                print(f"PatchTST training error for {column}: {str(e)[:120]}")
            continue

    if not errors:
        return {"MAE": float("nan"), "RMSE": float("nan"), "MAPE%": float("nan"), "R2": float("nan"), "n_tests": 0}

    arr = np.array(errors)
    return {
        "MAE": float(np.mean(arr[:, 0])),
        "RMSE": float(np.mean(arr[:, 1])),
        "MAPE%": float(np.nanmean(arr[:, 2])),
        "R2": float(np.nanmean(arr[:, 3])),
        "n_tests": int(len(errors)),
    }

# Naive persistence baseline
def backtest_naive_persistence(df: pd.DataFrame, column: str, horizon_days: int, train_window_days: int) -> Dict[str, float]:
    series = df.dropna(subset=[column]).sort_values("ds").reset_index(drop=True)
    if len(series) < train_window_days + horizon_days + 1:
        return {"MAE": float("nan"), "RMSE": float("nan"), "MAPE%": float("nan"), "R2": float("nan"), "n_tests": 0}

    errors = []
    for cutoff in range(train_window_days, len(series) - horizon_days, 10):
        last_obs = series.iloc[cutoff - 1][column]
        y_pred = np.array([last_obs] * horizon_days)
        y_true = series.iloc[cutoff:cutoff + horizon_days][column].values

        mae_val  = float(np.mean(np.abs(y_true - y_pred)))
        rmse_val = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))
        mape_val = safe_mape(y_true, y_pred)

        ss_res = float(np.sum((y_true - y_pred) ** 2))
        ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))
        r2_val = float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float("nan")

        errors.append((mae_val, rmse_val, mape_val, r2_val))

    if not errors:
        return {"MAE": float("nan"), "RMSE": float("nan"), "MAPE%": float("nan"), "R2": float("nan"), "n_tests": 0}

    arr = np.array(errors)
    return {
        "MAE": float(np.mean(arr[:, 0])),
        "RMSE": float(np.mean(arr[:, 1])),
        "MAPE%": float(np.nanmean(arr[:, 2])),
        "R2": float(np.nanmean(arr[:, 3])),
        "n_tests": int(len(errors)),
    }


# Evaluate PatchTST vs a NaivePersistence baseline across all (country, horizon, param) combos.
def evaluate_all() -> pd.DataFrame:
    # Collect metric dicts here; converted to a DataFrame at the end.
    rows = []
    # Total iterations to size the overall progress bar (countries × horizons × params).
    total_iterations = len(COUNTRIES) * len(HORIZONS) * len(PARAMS)

    # Single progress bar covering the entire nested loop.
    with tqdm(total=total_iterations, desc="Overall Progress", ncols=100) as pbar:
        # Iterate through configured countries; label for reporting, key for data fetch.
        for label, key in COUNTRIES.items():
            # Console breadcrumb for easier log scanning by country.
            print(f"\nProcessing {label}...")
            # Fetch historical weather data (optionally including soil moisture) for this country once.
            df = fetch_weather_data(country=key, history_years=HISTORY_YEARS, include_soil_moisture=True)

            # Loop over forecast horizons (e.g., 3/7/14 days ahead).
            for horizon in HORIZONS:
                # Loop over each weather parameter to evaluate (e.g., precip, temp, wind, soil moisture).
                for param in PARAMS:
                    # Update the progress bar label with the current triplet for live feedback.
                    pbar.set_description(f"{label} - {param} - {horizon}d")

                    # Guard: if this parameter column is entirely NaN, skip modeling and record NaN metrics.
                    if df[param].isna().all():
                        # Add placeholder rows for both models to keep the output table complete.
                        for model_name in ["PatchTST", "NaivePersistence"]:
                            rows.append({
                                "country": label,
                                "param": param,
                                "horizon_days": horizon,
                                "model": model_name,
                                "MAE": float("nan"),
                                "RMSE": float("nan"),
                                "MAPE%": float("nan"),
                                "R2": float("nan"),
                                "n_tests": 0
                            })
                        # Advance overall progress and continue to next combo.
                        pbar.update(1)
                        continue

                    # Try PatchTST backtest; if it fails, capture the error and fallback to NaN metrics.
                    try:
                        # Rolling backtest for PatchTST with the given horizon and training window.
                        m_patchtst = backtest_patchtst(
                            df, param,
                            horizon_days=horizon,
                            train_window_days=TRAIN_WINDOW_DAYS
                        )
                    except Exception as e:
                        # Print the error for debugging, but don't stop the batch run.
                        print(f"Error with PatchTST for {label}-{param}-{horizon}d: {e}")
                        # Standardized placeholder metrics to preserve table shape.
                        m_patchtst = {
                            "MAE": float("nan"),
                            "RMSE": float("nan"),
                            "MAPE%": float("nan"),
                            "R2": float("nan"),
                            "n_tests": 0
                        }

                    # Compute naive persistence baseline under the same settings for comparison.
                    m_naive = backtest_naive_persistence(
                        df, param,
                        horizon_days=horizon,
                        train_window_days=TRAIN_WINDOW_DAYS
                    )

                    # Record PatchTST results with identifying metadata columns.
                    rows.append({
                        "country": label,
                        "param": param,
                        "horizon_days": horizon,
                        "model": "PatchTST",
                        **m_patchtst
                    })
                    # Record naive baseline results for side-by-side analysis.
                    rows.append({
                        "country": label,
                        "param": param,
                        "horizon_days": horizon,
                        "model": "NaivePersistence",
                        **m_naive
                    })
                    # Mark this (country, param, horizon) combination as completed.
                    pbar.update(1)

    # Build a tidy, sorted DataFrame for consistent downstream use and readability.
    return pd.DataFrame(rows).sort_values(
        ["country", "param", "horizon_days", "model"]
    ).reset_index(drop=True)

def print_summary_table(results_df: pd.DataFrame):
    print("\n=== Model Comparison Summary ===")
    summary = results_df.groupby(['model', 'horizon_days']).agg({
        'MAE': 'mean', 'RMSE': 'mean', 'MAPE%': 'mean', 'R2': 'mean', 'n_tests': 'sum'
    }).round(3)
    print(summary)

    print("\n=== Best Model by Horizon (lowest MAE) ===")
    for horizon in HORIZONS:
        horizon_data = results_df[results_df['horizon_days'] == horizon]
        if horizon_data.empty:
            print(f"{horizon}-day horizon: (no data)")
            continue
        best_mae = horizon_data.groupby('model')['MAE'].mean().idxmin()
        best_mae_val = horizon_data.groupby('model')['MAE'].mean().min()
        print(f"{horizon}-day horizon: {best_mae} (MAE: {best_mae_val:.3f})")

if __name__ == "__main__":
    out_path = Path("forecast_eval_results_patchtst.csv")
    print("Starting PatchTST weather forecast evaluation...")
    print(f"Countries: {list(COUNTRIES.keys())}")
    print(f"Parameters: {PARAMS}")
    print(f"Horizons: {HORIZONS} days")
    print(f"PatchTST config: input_size={PATCHTST_CONFIG['input_size']}, max_steps={PATCHTST_CONFIG['max_steps']}")
    print(f"History years: {HISTORY_YEARS}, Train window days: {TRAIN_WINDOW_DAYS}")

    results_df = evaluate_all()

    # Display options
    pd.set_option("display.float_format", lambda v: f"{v:0.3f}" if isinstance(v, float) and not math.isnan(v) else str(v))
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)

    print_summary_table(results_df)
    print("\n=== Detailed Results (first 25 rows) ===")
    print(results_df.head(25))

    results_df.to_csv(out_path, index=False)
    print(f"\nSaved results to: {out_path.resolve()}")

    total_tests = results_df['n_tests'].sum()
    successful_tests = results_df[results_df['n_tests'] > 0].shape[0]
    print(f"\nTotal backtests run: {total_tests}")
    print(f"Successful model evaluations: {successful_tests}/{len(results_df)}")t


import os, glob
base = "/content/drive/MyDrive/Models/patchtst"
print("Path exists?", os.path.exists(base))

# List a few levels to confirm the tree
for p in glob.glob(base + "/**/*", recursive=True)[:50]:
    print(p)

# 1) If a fake local /content/drive exists with files, rename it out of the way
import os, shutil

LOCAL_DRIVE = "/content/drive"
RESCUE_DIR  = "/content/drive_local"

if os.path.ismount(LOCAL_DRIVE):
    print("Already mounted.  skip to step 3.")
else:
    if os.path.exists(LOCAL_DRIVE) and os.listdir(LOCAL_DRIVE):
        print("Found local files under /content/drive — renaming to /content/drive_local ...")
        # Rename the whole tree so the mountpoint is empty
        if os.path.exists(RESCUE_DIR):
            shutil.rmtree(RESCUE_DIR, ignore_errors=True)
        shutil.move(LOCAL_DRIVE, RESCUE_DIR)
    else:
        print("No local files under /content/drive.")

#  Mount the  Google Drive 
from google.colab import drive
drive.mount(LOCAL_DRIVE, force_remount=True)
print("Mounted:", os.path.ismount(LOCAL_DRIVE))

#  Move  models into real Drive
import os, shutil, glob

SRC = "/content/drive_local/MyDrive/Models/patchtst"  
DST = "/content/drive/MyDrive/Models/patchtst"        
if os.path.exists(SRC):
    os.makedirs(os.path.dirname(DST), exist_ok=True)
    try:
        shutil.move(SRC, DST)
    except Exception as e:
        print("Move failed, copying instead:", e)
        if os.path.exists(DST):
            shutil.rmtree(DST, ignore_errors=True)
        shutil.copytree(SRC, DST)
        shutil.rmtree(SRC, ignore_errors=True)
    print("Models are now in REAL Google Drive at:", DST)
else:
    print("Could not find rescued models at:", SRC)

