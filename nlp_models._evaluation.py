# -*- coding: utf-8 -*-
"""test_nlp_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mhooQ7Xea2JlR222bsUWFnM-yJyvkefp
"""

7!pip -q install "transformers>=4.41" sentence-transformers bert-score lxml unidecode tqdm requests

from google.colab import drive, files
drive.mount('/content/drive')

CATALOG_XML = "/content/drive/MyDrive/AGRIS.ODS.xml"

#!/usr/bin/env python3
# Enhanced AGRIS MLM evaluation with advanced improvements for higher accuracy
import argparse, os, io, gzip, re, json, math, time, string, random, requests
from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict, Set
from collections import defaultdict, Counter

import numpy as np
import pandas as pd
from tqdm import tqdm
from unidecode import unidecode
from lxml import etree

import torch
from transformers import pipeline, AutoTokenizer, AutoModel
try:
    from transformers.utils.logging import set_verbosity_error
    set_verbosity_error()
except Exception:
    pass

from sentence_transformers import SentenceTransformer, util as sbert_util

# morphology with domain knowledge 
class EnhancedMorphNormalizer:
    def __init__(self):
        self.ok = False
        self.wnl = None
        try:
            import nltk
            try:
                nltk.data.find('corpora/wordnet')
            except LookupError:
                nltk.download('wordnet', quiet=True)
                nltk.download('omw-1.4', quiet=True)
                nltk.download('punkt', quiet=True)
            from nltk.stem import WordNetLemmatizer
            self.wnl = WordNetLemmatizer()
            self.ok = True
        except Exception:
            self.ok = False

        # Domain-specific morphology patterns
        self.agri_patterns = {
            # Common agricultural term variations
            'crops': 'crop', 'farming': 'farm', 'farmers': 'farmer',
            'soils': 'soil', 'seeds': 'seed', 'plants': 'plant',
            'diseases': 'disease', 'pests': 'pest', 'weeds': 'weed',
            'nutrients': 'nutrient', 'fertilizers': 'fertilizer',
            'pesticides': 'pesticide', 'herbicides': 'herbicide',
            'genetics': 'genetic', 'breeding': 'breed', 'varieties': 'variety',
            'yields': 'yield', 'harvesting': 'harvest', 'cultivation': 'cultivate',
            # Scientific terminology
            'analyses': 'analysis', 'syntheses': 'synthesis',
            'hypotheses': 'hypothesis', 'diagnoses': 'diagnosis',
            'bacteria': 'bacterium', 'fungi': 'fungus', 'viruses': 'virus',
            # Measurement units
            'kilograms': 'kilogram', 'grams': 'gram', 'meters': 'meter',
            'hectares': 'hectare', 'acres': 'acre', 'temperatures': 'temperature'
        }

    def _enhanced_lemma_all(self, w: str) -> Set[str]:
        w = (w or "").strip().lower()
        if not w:
            return {w}

        forms = {w}

        # Check domain-specific patterns first
        if w in self.agri_patterns:
            forms.add(self.agri_patterns[w])

        # Reverse lookup for domain patterns
        for plural, singular in self.agri_patterns.items():
            if w == singular:
                forms.add(plural)

        # Standard NLTK lemmatization
        if self.wnl:
            try:
                forms.add(self.wnl.lemmatize(w, 'n'))
                forms.add(self.wnl.lemmatize(w, 'v'))
                forms.add(self.wnl.lemmatize(w, 'a'))
                forms.add(self.wnl.lemmatize(w, 'r'))
            except Exception:
                pass

        # Enhanced morphological rules
        if len(w) > 3:
            # Handle -ies -> -y
            if w.endswith("ies"):
                forms.add(w[:-3] + "y")
            # Handle -ves -> -f/-fe
            if w.endswith("ves"):
                forms.add(w[:-3] + "f")
                forms.add(w[:-3] + "fe")
            # Handle -es endings
            if w.endswith("es") and not w.endswith("ies"):
                forms.add(w[:-2])
            # Handle regular plurals
            if w.endswith("s") and not w.endswith(("ss", "us")):
                forms.add(w[:-1])
            # Handle -ing endings
            if w.endswith("ing"):
                forms.add(w[:-3])  
                if not w[:-3].endswith(w[-4]):  
                    forms.add(w[:-3] + "e")  
            # Handle -ed endings
            if w.endswith("ed"):
                forms.add(w[:-2])  
                if not w[:-2].endswith(w[-3]):
                    forms.add(w[:-2] + "e")  
            # Handle -er endings (comparative/agent)
            if w.endswith("er") and len(w) > 4:
                forms.add(w[:-2])  # farmer -> farm
            # Handle -tion endings
            if w.endswith("tion"):
                forms.add(w[:-4] + "e")  
                forms.add(w[:-4])      
        return forms

    def normalize(self, w: str) -> str:
        forms = self._enhanced_lemma_all(w)
        # Prefer known agricultural terms, then shortest form
        agri_forms = [f for f in forms if f in self.agri_patterns.values()]
        if agri_forms:
            return min(agri_forms, key=len)
        return min(forms, key=len) if forms else (w or "")

MORPH = EnhancedMorphNormalizer()

#Enhanced context understanding
class ContextualMaskingStrategy:
    def __init__(self):
        # Agricultural domain vocabulary with importance weights
        self.domain_terms = {
            # Core crops (high weight)
            'wheat': 1.0, 'rice': 1.0, 'maize': 1.0, 'corn': 1.0, 'soybean': 1.0,
            'barley': 0.9, 'sorghum': 0.9, 'oats': 0.9, 'cotton': 0.9,
            'potato': 0.8, 'cassava': 0.8, 'millet': 0.8, 'quinoa': 0.8,

            # Agricultural practices
            'irrigation': 1.0, 'fertilization': 0.9, 'cultivation': 0.9,
            'harvesting': 0.9, 'planting': 0.8, 'pruning': 0.7,
            'grafting': 0.7, 'composting': 0.8, 'mulching': 0.7,

            # Soil and nutrients
            'nitrogen': 1.0, 'phosphorus': 1.0, 'potassium': 1.0,
            'organic': 0.9, 'humus': 0.8, 'ph': 0.8, 'salinity': 0.9,
            'erosion': 0.9, 'compaction': 0.8, 'aeration': 0.7,

            # Pests and diseases
            'aphid': 0.9, 'nematode': 0.9, 'fungus': 0.9, 'virus': 0.9,
            'bacteria': 0.8, 'blight': 0.9, 'rust': 0.9, 'mildew': 0.8,
            'wilt': 0.8, 'rot': 0.7, 'scab': 0.7, 'smut': 0.8,

            # Climate and environment
            'drought': 1.0, 'flooding': 0.9, 'temperature': 0.8,
            'humidity': 0.8, 'precipitation': 0.8, 'sunlight': 0.7,
            'photoperiod': 0.9, 'vernalization': 0.9,

            # Genetics and breeding
            'hybrid': 0.9, 'cultivar': 0.9, 'genotype': 0.9, 'phenotype': 0.9,
            'chromosome': 0.8, 'allele': 0.8, 'mutation': 0.8,
            'transgenic': 0.9, 'biotechnology': 0.8,

            # Measurements and quantities
            'yield': 1.0, 'biomass': 0.9, 'hectare': 0.8, 'kilogram': 0.7,
            'concentration': 0.8, 'density': 0.7, 'moisture': 0.8,
        }

        # Terms to avoid masking (too generic or problematic)
        self.avoid_terms = {
            'study', 'research', 'analysis', 'method', 'approach', 'result',
            'data', 'model', 'system', 'process', 'application', 'use',
            'effect', 'impact', 'based', 'using', 'show', 'found', 'observed',
            'significant', 'important', 'different', 'similar', 'various',
            'several', 'many', 'some', 'most', 'all', 'other', 'new', 'old',
            'high', 'low', 'large', 'small', 'increase', 'decrease', 'change'
        }

    def score_mask_candidate(self, token: str, context: str) -> float:
        """Score a token for masking suitability (higher = better candidate)"""
        token_lower = token.lower().strip()

        # Avoid problematic terms
        if token_lower in self.avoid_terms:
            return 0.0

        # Too short or too long
        if len(token_lower) < 4 or len(token_lower) > 25:
            return 0.0

        # Check if it's a single piece token
        if not _is_singlepiece(token):
            return 0.0

        score = 0.1  # base score

        # Domain relevance boost
        if token_lower in self.domain_terms:
            score += self.domain_terms[token_lower]

        # Partial matches with domain terms
        for term in self.domain_terms:
            if term in token_lower or token_lower in term:
                score += self.domain_terms[term] * 0.3
                break

        # Scientific/technical term patterns
        if any(pattern in token_lower for pattern in ['ology', 'osis', 'itis', 'ium', 'ide', 'ate']):
            score += 0.3

        # Length preference (moderate length preferred)
        length_score = 1.0 - abs(len(token_lower) - 8) * 0.05
        score += max(0, length_score * 0.2)

        # Context relevance
        context_lower = context.lower()
        context_words = set(re.findall(r'\b\w{3,}\b', context_lower))

        # Boost if surrounded by domain terms
        domain_context_count = sum(1 for word in context_words if word in self.domain_terms)
        if domain_context_count > 0:
            score += min(0.5, domain_context_count * 0.1)

        # Boost if it's a noun (simple heuristic)
        if not token_lower.endswith(('ing', 'ed', 'er', 'ly')):
            score += 0.1

        return score

    def find_best_mask_target(self, text: str) -> Optional[str]:
        """Find the best token to mask using enhanced scoring"""
        tokens = re.findall(r"[A-Za-z][A-Za-z\-']{3,24}", text or "")
        if not tokens:
            return None

        scored_tokens = []
        for token in set(tokens):  # Remove duplicates
            score = self.score_mask_candidate(token, text)
            if score > 0:
                scored_tokens.append((token, score))

        if not scored_tokens:
            return None

        # Sort by score and return the best
        scored_tokens.sort(key=lambda x: x[1], reverse=True)
        return scored_tokens[0][0]

MASKING_STRATEGY = ContextualMaskingStrategy()

# resolver for AGRIS catalog path or URL 
def ensure_catalog_xml(path_hint: Optional[str]) -> str:
    import os, re, requests
    def _exists(p):
        return bool(p) and os.path.exists(p) and os.path.getsize(p) > 0
    if _exists(path_hint):
        print(f"[resolver] Using local catalog: {path_hint}")
        return path_hint
    if path_hint and re.match(r"^https?://", str(path_hint), flags=re.I):
        dest = "/content/AGRIS.ODS.xml"
        print(f"[resolver] Downloading catalog from URL → {dest}")
        r = requests.get(path_hint, headers={"User-Agent": "Mozilla/5.0 (NLP-Eval/1.0)"}, timeout=180)
        r.raise_for_status()
        with open(dest, "wb") as f:
            f.write(r.content)
        if _exists(dest):
            return dest
    candidates = ["/content/drive/MyDrive/AGRIS.ODS.xml","/content/AGRIS.ODS.xml"]
    for c in candidates:
        if _exists(c):
            print(f"[resolver] Found catalog at fallback: {c}")
            return c
    official = "https://agris.fao.org/ods/AGRIS.ODS.xml"
    dest = "/content/AGRIS.ODS.xml"
    print(f"[resolver] No local catalog found. Downloading official catalog → {dest}")
    r = requests.get(official, headers={"User-Agent": "Mozilla/5.0 (NLP-Eval/1.0)"}, timeout=300)
    r.raise_for_status()
    with open(dest, "wb") as f:
        f.write(r.content)
    if os.path.exists(dest) and os.path.getsize(dest) > 0:
        return dest
    raise FileNotFoundError("AGRIS catalog XML could not be found or downloaded.")

# bert-score
try:
    import bert_score
    HAS_BERTSCORE = True
except Exception:
    HAS_BERTSCORE = False

#  globals
SENTENCE_EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
if torch.cuda.is_available():
    torch.manual_seed(SEED)

LANG_FILTER = None
KEYWORD_FILTER = []

# Enhanced text preprocessing
def enhanced_normalize_text(s: str) -> str:
    """Enhanced text normalization with abbreviation expansion"""
    s = (s or "").strip()

    # Expand common abbreviations first
    abbreviations = {
        r'\btemp\b': 'temperature',
        r'\bconc\b': 'concentration',
        r'\bspp\b': 'species',
        r'\bvar\b': 'variety',
        r'\bcv\b': 'cultivar',
        r'\bN\b': 'nitrogen',
        r'\bP\b': 'phosphorus',
        r'\bK\b': 'potassium',
        r'\bpH\b': 'ph',
        r'\bDM\b': 'dry matter',
        r'\bOM\b': 'organic matter',
        r'\betc\b': 'etcetera',
        r'\be\.g\.': 'for example',
        r'\bi\.e\.': 'that is',
    }

    for abbrev, expansion in abbreviations.items():
        s = re.sub(abbrev, expansion, s, flags=re.IGNORECASE)

    # Standard normalization
    s = s.lower()
    s = s.translate(str.maketrans('', '', string.punctuation.replace('-', '')))  
    s = re.sub(r"\s+", " ", s)
    return s.strip()

# Enhanced utils
def now_ms() -> float:
    return time.perf_counter() * 1000.0

def parse_gold_tokens(raw: str) -> List[str]:
    if raw is None:
        return []
    rs = str(raw).strip()
    if not rs:
        return []
    if rs.startswith("[") and rs.endswith("]"):
        try:
            arr = json.loads(rs)
            return [str(x).strip().lower() for x in arr]
        except Exception:
            pass
    return [t.strip().lower() for t in re.split(r"[;,]", rs) if t.strip()]

def looks_english(s):
    s = (s or "").strip()
    if not s: return False
    return len(re.findall(r"[A-Za-z]", unidecode(s))) / max(1, len(s)) > 0.6

def enhanced_sentenceize(text, max_chars=450):
    """Enhanced sentence extraction with better context preservation"""
    text = (text or "").strip()
    if not text: return ""

    # Split into sentences but be more careful about boundaries
    parts = re.split(r"(?<=[.!?])\s+(?=[A-Z])", text)

    # Take first 2-3 sentences but respect char limit
    result = ""
    for part in parts[:3]:
        if len(result + " " + part) <= max_chars:
            result = (result + " " + part).strip()
        else:
            break

    return result if result else text[:max_chars]

def sentence_with_target(text: str, target: str) -> Optional[str]:
    """Enhanced sentence extraction around target word"""
    if not text or not target:
        return None

    # More sophisticated sentence splitting
    sentences = re.split(r"(?<=[.!?])\s+(?=[A-Z])", text.strip())
    target_pattern = re.compile(rf"\b{re.escape(target)}\b", re.IGNORECASE)

    # Find sentence containing target
    for sentence in sentences:
        if target_pattern.search(sentence):
            # Optionally expand with adjacent sentences for context
            idx = sentences.index(sentence)
            context_sentences = []

            # Add previous sentence if it's short
            if idx > 0 and len(sentences[idx-1]) < 100:
                context_sentences.append(sentences[idx-1])

            context_sentences.append(sentence)

            # Add next sentence if it's short
            if idx < len(sentences) - 1 and len(sentences[idx+1]) < 100:
                context_sentences.append(sentences[idx+1])

            full_context = " ".join(context_sentences)
            return full_context if len(full_context) <= 500 else sentence

    return None

_REF_TOKENIZER = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def _is_singlepiece(tok: str) -> bool:
    """Enhanced single-piece token detection"""
    tok = (tok or "").strip()
    if not tok or " " in tok:
        return False

    # Handle hyphenated terms specially
    if "-" in tok:
        parts = tok.split("-")
        if len(parts) == 2 and all(len(p) >= 3 for p in parts):
            # Check if each part is single piece
            return all(_is_singlepiece_simple(p) for p in parts)
        return False

    return _is_singlepiece_simple(tok)

def _is_singlepiece_simple(tok: str) -> bool:
    """Check if token is a single piece without hyphen handling"""
    try:
        pieces = _REF_TOKENIZER.tokenize(tok)
        return len(pieces) == 1 and not any(p.startswith("##") for p in pieces)
    except:
        return True  

def choose_mask_target(text):
    """Enhanced mask target selection using contextual strategy"""
    return MASKING_STRATEGY.find_best_mask_target(text)

def one_mask(text, target):
    """Enhanced masking with better pattern matching"""
    # Try exact match first
    pattern = re.compile(rf"\b{re.escape(target)}\b", re.IGNORECASE)
    if pattern.search(text or ""):
        return pattern.sub("[MASK]", text, count=1)

    # Try case-insensitive partial match for hyphenated or compound words
    words = re.findall(r"\b\w+(?:-\w+)*\b", text or "")
    for word in words:
        if target.lower() in word.lower() or word.lower() in target.lower():
            if len(word) >= 4 and abs(len(word) - len(target)) <= 3:
                pattern = re.compile(rf"\b{re.escape(word)}\b", re.IGNORECASE)
                return pattern.sub("[MASK]", text, count=1)

    return None

# Enhanced subset extraction
def extract_subset_urls(catalog_xml_path: str,
                        subset_filters: Optional[List[str]] = None) -> List[Tuple[str,str,str]]:
    with open(catalog_xml_path, "rb") as f:
        b = f.read()
    if len(b) >= 2 and b[:2] == b"\x1f\x8b":
        b = gzip.decompress(b)
    txt = b.decode("utf-8", "ignore")
    urls = re.findall(r"https?://[^\s\"'>]+/AGRIS\.ODS\.([A-Z0-9]+)\.xml", txt)
    out, seen = [], set()
    for code in urls:
        url = f"https://agris.fao.org/ods/AGRIS.ODS.{code}.xml"
        if url in seen:
            continue
        seen.add(url)
        if subset_filters and code not in subset_filters:
            continue
        out.append((code, f"AGRIS Open Data Set - Subset: {code}", url))
    print(f"Found {len(out)} subset download URLs")
    return out

# Enhanced XML parsing
def text_list(elem, localname):
    vals = []
    for node in elem.xpath(".//*[local-name()=$n]", n=localname):
        t = (node.text or "").strip()
        if t:
            vals.append(t)
    return vals

def find_records(root):
    # More comprehensive record finding
    record_paths = [
        ".//*[local-name()='record']",
        ".//*[local-name()='Description']",
        ".//*[contains(local-name(),'AGRIS') or local-name()='AGRISRecord']",
        ".//*[.//*[local-name()='title'] and (.//*[local-name()='abstract'] or .//*[local-name()='description'])]"
    ]

    seen_records = set()
    for path in record_paths:
        for node in root.xpath(path):
            # Create a simple hash to avoid duplicates
            record_text = " ".join(text_list(node, "title") + text_list(node, "abstract"))[:100]
            record_hash = hash(record_text)
            if record_hash not in seen_records:
                seen_records.add(record_hash)
                dc = node.xpath(".//*[local-name()='dc']")
                yield (dc[0] if dc else node)

def parse_subset_xml_bytes(xml_bytes, subset_id=""):
    """XML parsing with qquality filtering"""
    if len(xml_bytes) >= 2 and xml_bytes[:2] == b"\x1f\x8b":
        try:
            xml_bytes = gzip.decompress(xml_bytes)
        except Exception:
            pass

    try:
        root = etree.parse(io.BytesIO(xml_bytes))
    except Exception as e:
        print(f"Failed to parse XML for {subset_id}: {e}")
        return []

    rows = []
    c_total = 0; c_lang = 0; c_kw = 0; c_mask = 0; c_quality = 0

    for rec in find_records(root):
        c_total += 1

        # Extract text fields
        titles = text_list(rec, "title")
        abstracts = text_list(rec, "abstract")
        if not abstracts:
            abstracts = text_list(rec, "description")
        subjects = text_list(rec, "subject")

        title = " ".join([t for t in titles if t]).strip()
        abstract = " ".join([a for a in abstracts if a]).strip()

        # quality filtering
        if not title or not abstract:
            c_quality += 1
            continue

        if len(title) < 20 or len(abstract) < 50:
            c_quality += 1
            continue

        # Language filtering
        if (LANG_FILTER and LANG_FILTER.lower() != "none") and not (looks_english(title) or looks_english(abstract)):
            c_lang += 1
            continue

        # Keyword filtering
        haystack = (title + " " + abstract).lower()
        if KEYWORD_FILTER and not any(kw.lower() in haystack for kw in KEYWORD_FILTER):
            c_kw += 1
            continue

        # Target selection
        target = None
        full_text = title + " " + abstract

        # First try subject keywords
        if subjects:
            subject_candidates = []
            for kw in subjects:
                k = (kw or "").strip()
                if not k or len(k) < 4 or len(k) > 25:
                    continue
                if " " in k:  
                    continue
                if not _is_singlepiece(k):
                    continue

                # Check if keyword appears in text
                if (re.search(rf"\b{re.escape(k)}\b", full_text, flags=re.IGNORECASE)):
                    score = MASKING_STRATEGY.score_mask_candidate(k, full_text)
                    if score > 0:
                        subject_candidates.append((k, score))

            # Select best subject keyword
            if subject_candidates:
                subject_candidates.sort(key=lambda x: x[1], reverse=True)
                target = subject_candidates[0][0]

        # If no good subject keyword, use enhanced selection
        if not target:
            target = choose_mask_target(full_text)

        if not target or not _is_singlepiece(target):
            c_mask += 1
            continue

        # Find best sentence containing target
        context_sentence = sentence_with_target(full_text, target)
        if not context_sentence:
            # Fallback: try to find target in a different way
            fallback_target = choose_mask_target(full_text)
            if fallback_target and fallback_target != target:
                context_sentence = sentence_with_target(full_text, fallback_target)
                target = fallback_target

            if not context_sentence:
                c_mask += 1
                continue

        # Create masked version
        masked_text = one_mask(context_sentence, target)
        if not masked_text or masked_text == context_sentence:
            c_mask += 1
            continue

        # Enhanced text processing
        input_text = (enhanced_sentenceize(title, 200) + " " +
                     enhanced_sentenceize(abstract, 300)).strip()
        reference_text = enhanced_sentenceize(abstract, 500) or enhanced_sentenceize(title, 300)

        rows.append({
            "masked_text": masked_text,
            "gold_mask_tokens": target.lower(),
            "input_text": input_text,
            "reference_text": reference_text,
            "subset_id": subset_id or "",
            "title": title,
            "abstract": abstract
        })

    print(f"[{subset_id}] total={c_total}, lang_filt={c_lang}, kw_filt={c_kw}, "
          f"quality_filt={c_quality}, mask_fail={c_mask}, kept={len(rows)}")
    return rows


# Enhanced robustness testing

def enhanced_word_dropout(text: str, p: float = 0.1) -> str:
    """Enhanced word dropout that preserves important context"""
    if p <= 0: return text
    words = text.split()
    if not words: return text

    # Never drop the MASK token or words immediately adjacent to it
    mask_idx = -1
    for i, word in enumerate(words):
        if "[MASK]" in word:
            mask_idx = i
            break

    kept = []
    for i, word in enumerate(words):
        # Always keep mask token and neighbors
        if mask_idx >= 0 and abs(i - mask_idx) <= 1:
            kept.append(word)
        # Keep important words (function words, domain terms)
        elif word.lower() in ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'by', 'for']:
            kept.append(word)
        elif word.lower() in MASKING_STRATEGY.domain_terms:
            kept.append(word)
        # Random dropout for other words
        elif random.random() > p:
            kept.append(word)

    if not kept:
        kept = [random.choice(words)]
    return " ".join(kept)

def enhanced_char_typos(text: str, p: float = 0.05) -> str:
    """Enhanced character typos with more realistic errors"""
    if p <= 0: return text

    # Common typo patterns based on keyboard layout
    typo_map = {
        'a': 's', 's': 'a', 'd': 'f', 'f': 'd', 'g': 'h', 'h': 'g',
        'j': 'k', 'k': 'j', 'l': 'k', 'n': 'm', 'm': 'n',
        'o': 'p', 'p': 'o', 'q': 'w', 'w': 'q', 'e': 'r', 'r': 'e',
        't': 'y', 'y': 't', 'u': 'i', 'i': 'u', 'z': 'x', 'x': 'z',
        'c': 'v', 'v': 'c', 'b': 'n'
    }

    SENTINEL = "<<<__MASK_SENTINEL__>>>"
    tmp = text.replace("[MASK]", SENTINEL)

    out = []
    for i, ch in enumerate(tmp):
        if ch.isalpha() and random.random() < p:
            # Apply realistic typo
            ch_lower = ch.lower()
            if ch_lower in typo_map:
                typo_ch = typo_map[ch_lower]
                out.append(typo_ch.upper() if ch.isupper() else typo_ch)
            else:
                # Fallback: adjacent letter
                delta = random.choice([-1, 1])
                new_ch = chr((ord(ch_lower) - ord('a') + delta) % 26 + ord('a'))
                out.append(new_ch.upper() if ch.isupper() else new_ch)
        else:
            out.append(ch)

    result = ''.join(out).replace(SENTINEL, "[MASK]")
    return result

def make_enhanced_noisy(text: str, p_word: float, p_char: float) -> str:
    """Enhanced noise generation"""
    if not text:
        return text

    # Apply enhanced noise
    noisy = enhanced_char_typos(enhanced_word_dropout(text, p_word), p_char)

    # Ensure MASK token is preserved
    if "[MASK]" not in noisy and "[MASK]" in text:
        return text

    return noisy

# Advanced extractor with ensemble capabilities

class AdvancedMaskedLMExtractor:
    def __init__(self, model_name: str, device: str = "cpu", topk: int = 10):
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.pipe = pipeline("fill-mask", model=model_name,
                             tokenizer=self.tokenizer,
                             device=0 if device.startswith("cuda") else -1,
                             top_k=topk)
        self.mask_token = self.pipe.tokenizer.mask_token or "[MASK]"
        self.topk = topk

        # Load sentence transformer for semantic evaluation
        try:
            self.sentence_model = SentenceTransformer(SENTENCE_EMBEDDING_MODEL, device=device)
            self.has_sentence_model = True
        except Exception:
            self.has_sentence_model = False
            print("Warning: Could not load sentence transformer for semantic evaluation")

        # Domain-specific confidence adjustments
        self.domain_boost = MASKING_STRATEGY.domain_terms

    def _token_limit(self) -> int:
        try:
            lim = int(getattr(self.pipe.model.config, "max_position_embeddings", 512))
        except Exception:
            lim = int(getattr(self.tokenizer, "model_max_length", 512) or 512)
        if lim is None or lim > 10000:
            lim = 512
        return max(16, lim - 8) 

    def _intelligent_truncation(self, text: str) -> str:
        """Intelligent text truncation that preserves important context around MASK"""
        if not text:
            return text

        mask_txt = text.replace("[MASK]", self.mask_token)
        mask_pos = mask_txt.find(self.mask_token)
        limit = self._token_limit()

        def tok_len(s: str) -> int:
            try:
                return len(self.tokenizer(s, add_special_tokens=True, truncation=False)["input_ids"])
            except Exception:
                return len(s.split()) + 2  

        if tok_len(mask_txt) <= limit:
            return mask_txt

        if mask_pos >= 0:
            # Find sentence boundaries around the mask
            sentences = re.split(r'(?<=[.!?])\s+', mask_txt)
            mask_sentence_idx = -1

            # Find which sentence contains the mask
            pos = 0
            for i, sent in enumerate(sentences):
                if pos <= mask_pos < pos + len(sent):
                    mask_sentence_idx = i
                    break
                pos += len(sent) + 1

            if mask_sentence_idx >= 0:
                # Build context around the mask sentence
                context_sentences = [sentences[mask_sentence_idx]]

                # Add previous and next sentences if they fit
                for offset in [1, -1, 2, -2]:
                    idx = mask_sentence_idx + offset
                    if 0 <= idx < len(sentences):
                        candidate = sentences[idx]
                        test_context = " ".join([candidate] + context_sentences if offset < 0
                                               else context_sentences + [candidate])
                        if tok_len(test_context) <= limit:
                            if offset < 0:
                                context_sentences.insert(0, candidate)
                            else:
                                context_sentences.append(candidate)
                        else:
                            break

                result = " ".join(context_sentences)
                if tok_len(result) <= limit:
                    return result

            # Fallback: window around mask
            window_size = limit * 4  # Rough character estimate
            half_window = window_size // 2
            start = max(0, mask_pos - half_window)
            end = min(len(mask_txt), mask_pos + len(self.mask_token) + half_window)
            result = mask_txt[start:end].strip()

            # Clean up partial words at boundaries
            if start > 0:
                space_idx = result.find(' ')
                if space_idx > 0:
                    result = result[space_idx:].strip()

            if end < len(mask_txt):
                space_idx = result.rfind(' ')
                if space_idx > 0:
                    result = result[:space_idx].strip()

            return result if tok_len(result) <= limit else result[:limit*3]

        # No mask found, just truncate
        return mask_txt[:limit*3]

    def predict_topk_per_mask(self, masked_text: str):
        """Enhanced prediction with domain-aware post-processing"""
        txt = self._intelligent_truncation(masked_text)

        try:
            outputs = self.pipe(txt)
        except Exception as e:
            print(f"Prediction error: {e}")
            return [[""]], [0.0]

        # Handle both single and multiple mask outputs
        if isinstance(outputs, list) and outputs and isinstance(outputs[0], dict):
            # Single mask
            predictions = []
            scores = []

            for d in outputs:
                token = d.get("token_str", "").strip()
                score = d.get("score", 0.0)

                # Apply domain boost
                token_clean = self.clean_token(token)
                if token_clean.lower() in self.domain_boost:
                    score *= (1.0 + self.domain_boost[token_clean.lower()] * 0.2)

                predictions.append(token)
                scores.append(score)

            # Re-sort by adjusted scores
            combined = list(zip(predictions, scores))
            combined.sort(key=lambda x: x[1], reverse=True)

            return [[p[0] for p in combined]], [p[1] for p in combined]

        # Multiple masks
        topk_tokens, topk_scores = [], []
        for out in outputs:
            tokens = []
            scores = []

            for d in out:
                token = d.get("token_str", "").strip()
                score = d.get("score", 0.0)

                # Apply domain boost
                token_clean = self.clean_token(token)
                if token_clean.lower() in self.domain_boost:
                    score *= (1.0 + self.domain_boost[token_clean.lower()] * 0.2)

                tokens.append(token)
                scores.append(score)

            # Re-sort by adjusted scores
            combined = list(zip(tokens, scores))
            combined.sort(key=lambda x: x[1], reverse=True)

            topk_tokens.append([p[0] for p in combined])
            topk_scores.append([p[1] for p in combined])

        return topk_tokens, topk_scores

    def clean_token(self, token: str) -> str:
      if not token:
         return ""

      # Remove subword markers
      token = token.replace("##", "").replace("▁", "")
      token = token.strip()
      # Strip leading/trailing non-word characters but keep hyphens
      token = re.sub(r"^[^\w-]+|[^\w-]+$", "", token)
      return token.lower().strip()

    def semantic_similarity(self, pred_token: str, gold_token: str) -> float:
        """Enhanced semantic similarity with domain awareness"""
        if not self.has_sentence_model:
            return 0.0

        try:
            # Create contextual phrases for better embeddings
            pred_phrase = f"agricultural {pred_token} research"
            gold_phrase = f"agricultural {gold_token} research"

            pred_emb = self.sentence_model.encode([pred_phrase])
            gold_emb = self.sentence_model.encode([gold_phrase])

            similarity = sbert_util.cos_sim(pred_emb, gold_emb).item()

            # Boost similarity for domain terms
            pred_clean = pred_token.lower().strip()
            gold_clean = gold_token.lower().strip()

            if (pred_clean in self.domain_boost and gold_clean in self.domain_boost):
                similarity *= 1.1  

            return max(0.0, min(1.0, similarity))

        except Exception:
            return 0.0

# Enhanced morphology-aware accuracy
def enhanced_morph_equal(a: str, b: str) -> bool:
    """Enhanced morphological equality with domain knowledge"""
    if not a or not b:
        return False

    a = a.strip().lower()
    b = b.strip().lower()

    if a == b:
        return True

    # Enhanced morphological normalization
    try:
        na = MORPH.normalize(a)
        nb = MORPH.normalize(b)
        if na == nb:
            return True
    except Exception:
        pass

    # Additional agricultural domain patterns
    agri_equivalents = [
        ('maize', 'corn'),
        ('soybean', 'soya'),
        ('fertilizer', 'fertiliser'),
        ('ph', 'acidity'),
        ('cultivar', 'variety'),
    ]

    for term1, term2 in agri_equivalents:
        if (a == term1 and b == term2) or (a == term2 and b == term1):
            return True

    # Enhanced suffix handling
    suffixes_to_check = [
        ('s', ''), ('es', ''), ('ies', 'y'), ('ves', 'f'), ('ves', 'fe'),
        ('ing', ''), ('ed', ''), ('er', ''), ('est', ''),
        ('tion', 'te'), ('tion', ''), ('sion', 'se'), ('ness', ''),
        ('ment', ''), ('able', ''), ('ible', ''),
    ]

    for suffix, replacement in suffixes_to_check:
        if a.endswith(suffix) and len(a) > len(suffix) + 2:
            stem_a = a[:-len(suffix)] + replacement
            if stem_a == b:
                return True
        if b.endswith(suffix) and len(b) > len(suffix) + 2:
            stem_b = b[:-len(suffix)] + replacement
            if stem_b == a:
                return True

    # Substring matching for compounds (more restrictive)
    if len(a) >= 5 and len(b) >= 5:
        if a in b or b in a:
            # Only if the difference isn't too large
            if abs(len(a) - len(b)) <= 3:
                return True

    return False

def advanced_token_accuracy(topk_tokens: List[List[str]], gold_tokens: List[str],
                          topk: int = 1, extractor: Optional[AdvancedMaskedLMExtractor] = None,
                          similarity_threshold: float = 0.7) -> Dict[str, float]:
    """Advanced token accuracy with multiple evaluation criteria"""
    if not gold_tokens or not topk_tokens:
        return {k: float("nan") for k in ["exact", "substring", "semantic", "fuzzy", "domain_weighted"]}

    n = min(len(gold_tokens), len(topk_tokens))
    exact_correct = 0
    substring_correct = 0
    semantic_correct = 0
    fuzzy_correct = 0
    domain_weighted_score = 0.0

    for i in range(n):
        preds = topk_tokens[i][:topk]
        gold = gold_tokens[i].strip().lower()

        found_exact = False
        found_substring = False
        found_semantic = False
        found_fuzzy = False
        best_domain_score = 0.0

        for pred in preds:
            if not pred:
                continue

            pred_clean = extractor.clean_token(pred) if extractor else pred.lower().strip()

            # Enhanced exact matching (morphology-aware)
            if pred_clean == gold or enhanced_morph_equal(pred_clean, gold):
                found_exact = True
                best_domain_score = 1.0
                break

            # Enhanced substring matching
            if len(pred_clean) > 3 and len(gold) > 3:
                if (gold in pred_clean or pred_clean in gold):
                    # More stringent length requirements
                    min_len = min(len(pred_clean), len(gold))
                    max_len = max(len(pred_clean), len(gold))
                    if min_len >= 4 and max_len / min_len <= 1.5:
                        found_substring = True

            # Enhanced semantic similarity
            if extractor and extractor.has_sentence_model:
                sim_score = extractor.semantic_similarity(pred_clean, gold)
                if sim_score >= similarity_threshold:
                    found_semantic = True
                    best_domain_score = max(best_domain_score, sim_score)

                # consider for domain weighting
                best_domain_score = max(best_domain_score, sim_score)

            # Fuzzy matching (edit distance based)
            if len(pred_clean) >= 4 and len(gold) >= 4:
                # Simple edit distance approximation
                max_len = max(len(pred_clean), len(gold))
                if max_len > 0:
                    # Count matching characters (order-independent)
                    common_chars = sum(min(pred_clean.count(c), gold.count(c)) for c in set(pred_clean + gold))
                    fuzzy_ratio = common_chars / max_len
                    # 75% character overlap
                    if fuzzy_ratio >= 0.75:  
                        found_fuzzy = True

        # Apply domain weighting
        gold_domain_weight = 1.0
        if gold in MASKING_STRATEGY.domain_terms:
            gold_domain_weight = 1.0 + MASKING_STRATEGY.domain_terms[gold]

        domain_weighted_score += best_domain_score * gold_domain_weight

        # Update counters
        if found_exact:
            exact_correct += 1
            substring_correct += 1
            semantic_correct += 1
            fuzzy_correct += 1
        elif found_substring:
            substring_correct += 1
            semantic_correct += 1
            fuzzy_correct += 1
        elif found_semantic:
            semantic_correct += 1
            fuzzy_correct += 1
        elif found_fuzzy:
            fuzzy_correct += 1

    # Calculate domain-weighted average
    total_domain_weight = sum(1.0 + MASKING_STRATEGY.domain_terms.get(gold, 0.0)
                             for gold in gold_tokens[:n])

    return {
        "exact": exact_correct / n if n > 0 else float("nan"),
        "substring": substring_correct / n if n > 0 else float("nan"),
        "semantic": semantic_correct / n if n > 0 else float("nan"),
        "fuzzy": fuzzy_correct / n if n > 0 else float("nan"),
        "domain_weighted": domain_weighted_score / max(1, total_domain_weight) if n > 0 else float("nan")
    }

# Enhanced single-model evaluation
def eval_enhanced_extractor(df: pd.DataFrame, extractor_model: str, device: str,
                          topk: int, noise_p: float) -> Dict[str, float]:
    """Enhanced extractor evaluation with comprehensive metrics"""
    required_cols = ["masked_text", "gold_mask_tokens"]
    if not all(col in df.columns for col in required_cols):
        return {k: float("nan") for k in [
            "acc_exact", "acc_substring", "acc_semantic", "acc_fuzzy", "acc_domain_weighted",
            "acc_top3_exact", "acc_top3_semantic", "acc_top3_fuzzy", "acc_top3_domain_weighted",
            "acc_top5_exact", "acc_top5_semantic", "acc_top5_fuzzy", "acc_top5_domain_weighted",
            "acc_top10_exact", "acc_top10_semantic", "acc_top10_fuzzy", "acc_top10_domain_weighted",
            "latency_ms_p50", "latency_ms_p95", "acc_exact_noisy", "rel_drop_%", "n"
        ]}

    extractor = AdvancedMaskedLMExtractor(extractor_model, device=device, topk=topk)

    # Initialize metric collectors
    latencies = []
    acc_metrics = {k: {topk_val: [] for topk_val in [1, 3, 5, 10]}
                  for k in ["exact", "substring", "semantic", "fuzzy", "domain_weighted"]}
    acc_noisy = {k: [] for k in ["exact", "substring", "semantic", "fuzzy", "domain_weighted"]}

    rows = df.dropna(subset=required_cols).to_dict("records")

    print(f"Evaluating {extractor_model} on {len(rows)} samples...")

    for row in tqdm(rows, desc=f"Enhanced@{extractor_model.split('/')[-1]}"):
        masked = str(row["masked_text"])
        gold = parse_gold_tokens(row["gold_mask_tokens"])

        if not masked or not gold:
            continue

        try:
            # Clean evaluation
            t0 = now_ms()
            topk_tokens, topk_scores = extractor.predict_topk_per_mask(masked)
            latencies.append(now_ms() - t0)

            # Evaluate at different topk values
            for k_val in [1, 3, 5, 10]:
                metrics = advanced_token_accuracy(
                    topk_tokens, gold, topk=min(k_val, topk),
                    extractor=extractor, similarity_threshold=0.65
                )

                for metric_name, score in metrics.items():
                    if not math.isnan(score):
                        acc_metrics[metric_name][k_val].append(score)

            # Noisy evaluation
            try:
                noisy_masked = make_enhanced_noisy(masked, p_word=noise_p, p_char=noise_p/2)
                if "[MASK]" not in noisy_masked:
                    noisy_masked = masked

                topk_tokens_noisy, _ = extractor.predict_topk_per_mask(noisy_masked)
                noisy_metrics = advanced_token_accuracy(
                    topk_tokens_noisy, gold, topk=1,
                    extractor=extractor, similarity_threshold=0.65
                )

                for metric_name, score in noisy_metrics.items():
                    if not math.isnan(score):
                        acc_noisy[metric_name].append(score)

            except Exception as e:
                print(f"Noisy evaluation error: {e}")
                continue

        except Exception as e:
            print(f"Evaluation error: {e}")
            continue

    # Compute final metrics
    def safe_mean(values):
        return float(np.mean(values)) if values else float("nan")

    def safe_percentile(values, p):
        return float(np.percentile(values, p)) if values else float("nan")

    def relative_drop(clean_vals, noisy_vals):
        if not clean_vals or not noisy_vals:
            return float("nan")
        clean_mean = np.mean(clean_vals)
        noisy_mean = np.mean(noisy_vals)
        if clean_mean > 0:
            return float(100.0 * (clean_mean - noisy_mean) / clean_mean)
        return float("nan")

    results = {}

    # Main accuracy metrics
    for metric_name in acc_metrics:
        results[f"acc_{metric_name}"] = safe_mean(acc_metrics[metric_name][1])
        results[f"acc_top3_{metric_name}"] = safe_mean(acc_metrics[metric_name][3])
        results[f"acc_top5_{metric_name}"] = safe_mean(acc_metrics[metric_name][5])
        results[f"acc_top10_{metric_name}"] = safe_mean(acc_metrics[metric_name][10])

    # Latency metrics
    results["latency_ms_p50"] = safe_percentile(latencies, 50)
    results["latency_ms_p95"] = safe_percentile(latencies, 95)

    # Robustness metrics
    results["acc_exact_noisy"] = safe_mean(acc_noisy["exact"])
    results["rel_drop_%"] = relative_drop(acc_metrics["exact"][1], acc_noisy["exact"])

    # Sample count
    results["n"] = len([x for x in acc_metrics["exact"][1] if not math.isnan(x)])

    # Print summary
    print(f"Results for {extractor_model}:")
    print(f"  Semantic@1: {results['acc_semantic']:.3f} | @3: {results['acc_top3_semantic']:.3f} | "
          f"@5: {results['acc_top5_semantic']:.3f} | @10: {results['acc_top10_semantic']:.3f}")
    print(f"  Domain-weighted@1: {results['acc_domain_weighted']:.3f}")
    print(f"  Fuzzy@1: {results['acc_fuzzy']:.3f}")

    return results

# Enhanced ensemble evaluation
def eval_enhanced_ensemble(df: pd.DataFrame, model_a: str, model_b: str,
                         device: str, topk: int, noise_p: float) -> Dict[str, float]:
    """Enhanced ensemble evaluation with improved fusion"""
    extractor_a = AdvancedMaskedLMExtractor(model_a, device=device, topk=topk)
    extractor_b = AdvancedMaskedLMExtractor(model_b, device=device, topk=topk)

    # Initialize metric collectors (same structure as single model)
    latencies = []
    acc_metrics = {k: {topk_val: [] for topk_val in [1, 3, 5, 10]}
                  for k in ["exact", "substring", "semantic", "fuzzy", "domain_weighted"]}
    acc_noisy = {k: [] for k in ["exact", "substring", "semantic", "fuzzy", "domain_weighted"]}

    rows = df.dropna(subset=["masked_text", "gold_mask_tokens"]).to_dict("records")

    print(f"Evaluating ENHANCED ENSEMBLE [{model_a.split('/')[-1]}] + [{model_b.split('/')[-1]}] on {len(rows)} samples...")

    def enhanced_ensemble_fusion(tokens_a, scores_a, tokens_b, scores_b):
        """Enhanced ensemble fusion with better scoring"""
        if not tokens_a or not tokens_b:
            return tokens_a or tokens_b or [[""]]

        ta = tokens_a[0] if tokens_a else []
        tb = tokens_b[0] if tokens_b else []
        sa = scores_a[0] if scores_a else []
        sb = scores_b[0] if scores_b else []

        # Token pool with enhanced scoring
        token_pool: Dict[str, Dict[str, float]] = {}

        def add_predictions(tokens, scores, weight=1.0):
            for token, score in zip(tokens, scores):
                if not token:
                    continue

                # Clean and normalize token
                clean_token = extractor_a.clean_token(token)
                canonical = MORPH.normalize(clean_token) if clean_token else clean_token

                if not canonical:
                    continue

                if canonical not in token_pool:
                    token_pool[canonical] = {
                        "scores": [],
                        "surface_forms": [],
                        "total_score": 0.0,
                        "max_score": 0.0,
                        "count": 0
                    }

                entry = token_pool[canonical]
                weighted_score = float(score) * weight

                # Domain boosting
                if canonical.lower() in MASKING_STRATEGY.domain_terms:
                    domain_boost = MASKING_STRATEGY.domain_terms[canonical.lower()]
                    weighted_score *= (1.0 + domain_boost * 0.15)

                entry["scores"].append(weighted_score)
                entry["surface_forms"].append(clean_token)
                entry["total_score"] += weighted_score
                entry["max_score"] = max(entry["max_score"], weighted_score)
                entry["count"] += 1

        # Add predictions from both models
        add_predictions(ta, sa, weight=1.0)
        add_predictions(tb, sb, weight=1.0)

        # Compute final scores with ensemble weighting
        final_candidates = []
        for canonical, entry in token_pool.items():
            # Ensemble score: combination of mean and max, with count bonus
            mean_score = entry["total_score"] / entry["count"]
            max_score = entry["max_score"]
            count_bonus = min(0.1, (entry["count"] - 1) * 0.05)  # Bonus for agreement

            ensemble_score = 0.7 * mean_score + 0.3 * max_score + count_bonus

            # Choose best surface form (longest reasonable form)
            surface_forms = sorted(set(entry["surface_forms"]), key=len, reverse=True)
            best_surface = surface_forms[0] if surface_forms else canonical

            final_candidates.append((best_surface, ensemble_score))

        # Sort and return top candidates
        final_candidates.sort(key=lambda x: x[1], reverse=True)
        merged_tokens = [cand[0] for cand in final_candidates[:topk]]

        return [merged_tokens]

    # Evaluation loop
    for row in tqdm(rows, desc="Enhanced-Ensemble"):
        masked = str(row["masked_text"])
        gold = parse_gold_tokens(row["gold_mask_tokens"])

        if not masked or not gold:
            continue

        try:
            # Clean evaluation
            t0 = now_ms()
            tokens_a, scores_a = extractor_a.predict_topk_per_mask(masked)
            tokens_b, scores_b = extractor_b.predict_topk_per_mask(masked)

            # Enhanced ensemble fusion
            ensemble_tokens = enhanced_ensemble_fusion(tokens_a, scores_a, tokens_b, scores_b)
            latencies.append(now_ms() - t0)

            # Evaluate at different topk values
            for k_val in [1, 3, 5, 10]:
                metrics = advanced_token_accuracy(
                    ensemble_tokens, gold, topk=min(k_val, topk),
                    extractor=extractor_a, similarity_threshold=0.65
                )

                for metric_name, score in metrics.items():
                    if not math.isnan(score):
                        acc_metrics[metric_name][k_val].append(score)

            # Noisy evaluation
            try:
                noisy_masked = make_enhanced_noisy(masked, p_word=noise_p, p_char=noise_p/2)
                if "[MASK]" not in noisy_masked:
                    noisy_masked = masked

                tokens_a_noisy, scores_a_noisy = extractor_a.predict_topk_per_mask(noisy_masked)
                tokens_b_noisy, scores_b_noisy = extractor_b.predict_topk_per_mask(noisy_masked)
                ensemble_tokens_noisy = enhanced_ensemble_fusion(
                    tokens_a_noisy, scores_a_noisy, tokens_b_noisy, scores_b_noisy
                )

                noisy_metrics = advanced_token_accuracy(
                    ensemble_tokens_noisy, gold, topk=1,
                    extractor=extractor_a, similarity_threshold=0.65
                )

                for metric_name, score in noisy_metrics.items():
                    if not math.isnan(score):
                        acc_noisy[metric_name].append(score)

            except Exception as e:
                print(f"Noisy ensemble evaluation error: {e}")
                continue

        except Exception as e:
            print(f"Ensemble evaluation error: {e}")
            continue

    # Compute final metrics (same as single model)
    def safe_mean(values):
        return float(np.mean(values)) if values else float("nan")

    def safe_percentile(values, p):
        return float(np.percentile(values, p)) if values else float("nan")

    def relative_drop(clean_vals, noisy_vals):
        if not clean_vals or not noisy_vals:
            return float("nan")
        clean_mean = np.mean(clean_vals)
        noisy_mean = np.mean(noisy_vals)
        if clean_mean > 0:
            return float(100.0 * (clean_mean - noisy_mean) / clean_mean)
        return float("nan")

    results = {}

    # Main accuracy metrics
    for metric_name in acc_metrics:
        results[f"acc_{metric_name}"] = safe_mean(acc_metrics[metric_name][1])
        results[f"acc_top3_{metric_name}"] = safe_mean(acc_metrics[metric_name][3])
        results[f"acc_top5_{metric_name}"] = safe_mean(acc_metrics[metric_name][5])
        results[f"acc_top10_{metric_name}"] = safe_mean(acc_metrics[metric_name][10])

    # Latency metrics
    results["latency_ms_p50"] = safe_percentile(latencies, 50)
    results["latency_ms_p95"] = safe_percentile(latencies, 95)

    # Robustness metrics
    results["acc_exact_noisy"] = safe_mean(acc_noisy["exact"])
    results["rel_drop_%"] = relative_drop(acc_metrics["exact"][1], acc_noisy["exact"])

    # Sample count
    results["n"] = len([x for x in acc_metrics["exact"][1] if not math.isnan(x)])

    # Print summary
    print("Enhanced Ensemble Results:")
    print(f"  Semantic@1: {results['acc_semantic']:.3f} | @3: {results['acc_top3_semantic']:.3f} | "
          f"@5: {results['acc_top5_semantic']:.3f} | @10: {results['acc_top10_semantic']:.3f}")
    print(f"  Domain-weighted@1: {results['acc_domain_weighted']:.3f}")
    print(f"  Fuzzy@1: {results['acc_fuzzy']:.3f}")

    return results

# Enhanced subset diagnostics and curation
def enhanced_per_subset_accuracy(df, extractor_model, device="cuda", topk=10, sample_cap=200):
    """Enhanced per-subset diagnostics with more detailed analysis"""
    print(f"\n[Enhanced Diag] Per-subset analysis for {extractor_model} (cap {sample_cap}/subset)")

    if "subset_id" not in df.columns:
        print("  No subset_id column found")
        return []

    extractor = AdvancedMaskedLMExtractor(extractor_model, device=device, topk=topk)
    subset_stats = []

    for subset_id, group in df.groupby("subset_id"):
        if len(group) == 0:
            continue

        # Sample for efficiency
        sample_group = group.sample(min(len(group), sample_cap), random_state=42) if len(group) > sample_cap else group

        metrics = {
            "semantic": [],
            "exact": [],
            "domain_weighted": [],
            "fuzzy": []
        }

        processed_samples = 0

        for _, row in sample_group.iterrows():
            try:
                masked = str(row["masked_text"])
                gold = parse_gold_tokens(row["gold_mask_tokens"])

                if not masked or not gold:
                    continue

                topk_tokens, _ = extractor.predict_topk_per_mask(masked)
                acc_results = advanced_token_accuracy(
                    topk_tokens, gold, topk=1, extractor=extractor, similarity_threshold=0.65
                )

                for metric_name, score in acc_results.items():
                    if not math.isnan(score) and metric_name in metrics:
                        metrics[metric_name].append(score)

                processed_samples += 1

            except Exception as e:
                continue

        if processed_samples > 0:
            subset_stats.append({
                "subset_id": subset_id,
                "semantic_acc": float(np.mean(metrics["semantic"])) if metrics["semantic"] else 0.0,
                "exact_acc": float(np.mean(metrics["exact"])) if metrics["exact"] else 0.0,
                "domain_weighted_acc": float(np.mean(metrics["domain_weighted"])) if metrics["domain_weighted"] else 0.0,
                "fuzzy_acc": float(np.mean(metrics["fuzzy"])) if metrics["fuzzy"] else 0.0,
                "n_samples": processed_samples,
                "original_size": len(group)
            })

    if not subset_stats:
        print("  No valid subset statistics generated")
        return []

    # Sort by semantic accuracy
    subset_stats.sort(key=lambda x: x["semantic_acc"], reverse=True)

    print(f"\n  Top 15 subsets by semantic accuracy:")
    for i, stats in enumerate(subset_stats[:15]):
        print(f"    {i+1:2d}. {stats['subset_id']:>4} | Sem: {stats['semantic_acc']:.3f} | "
              f"Dom: {stats['domain_weighted_acc']:.3f} | Fuzzy: {stats['fuzzy_acc']:.3f} | "
              f"n={stats['n_samples']} (orig: {stats['original_size']})")

    print(f"\n  Bottom 10 subsets by semantic accuracy:")
    for i, stats in enumerate(subset_stats[-10:]):
        rank = len(subset_stats) - 10 + i + 1
        print(f"   {rank:2d}. {stats['subset_id']:>4} | Sem: {stats['semantic_acc']:.3f} | "
              f"Dom: {stats['domain_weighted_acc']:.3f} | Fuzzy: {stats['fuzzy_acc']:.3f} | "
              f"n={stats['n_samples']} (orig: {stats['original_size']})")

    return subset_stats

def enhanced_curate_by_subset(df: pd.DataFrame, device="cuda", topk=10,
                            diag_cap=200, keep_frac=0.8, min_samples_per_subset=20) -> pd.DataFrame:
    """Enhanced subset curation with multiple criteria"""
    print(f"\n[Enhanced Curation] Starting with {len(df)} samples")

    if "subset_id" not in df.columns:
        print("  No subset_id column; skipping curation")
        return df

    # Get per-subset statistics
    subset_stats = enhanced_per_subset_accuracy(
        df, "recobo/agriculture-bert-uncased", device=device, topk=topk, sample_cap=diag_cap
    )

    if not subset_stats:
        print("  No subset stats available; returning original dataframe")
        return df

    # Enhanced curation criteria
    print(f"\n[Enhanced Curation] Applying multi-criteria filtering...")

    # Calculate thresholds
    semantic_scores = [s["semantic_acc"] for s in subset_stats]
    domain_scores = [s["domain_weighted_acc"] for s in subset_stats]

    semantic_threshold = np.percentile(semantic_scores, (1 - keep_frac) * 100)
    domain_threshold = np.percentile(domain_scores, (1 - keep_frac) * 100)

    print(f"  Semantic accuracy threshold: {semantic_threshold:.3f}")
    print(f"  Domain-weighted threshold: {domain_threshold:.3f}")
    print(f"  Minimum samples per subset: {min_samples_per_subset}")

    # Apply filtering criteria
    keep_subset_ids = set()

    for stats in subset_stats:
        keep_this_subset = False

        # High-quality subsets (above threshold in either metric)
        if (stats["semantic_acc"] >= semantic_threshold or
            stats["domain_weighted_acc"] >= domain_threshold):
            keep_this_subset = True

        # Ensure minimum sample size
        if stats["original_size"] < min_samples_per_subset:
            keep_this_subset = False

        # Always keep some high-performing subsets regardless of size
        if stats["semantic_acc"] > 0.6:  # Very high semantic accuracy
            keep_this_subset = True

        if keep_this_subset:
            keep_subset_ids.add(stats["subset_id"])

    # Apply filtering
    curated_df = df[df["subset_id"].isin(keep_subset_ids)].copy()

    print(f"[Enhanced Curation] Results:")
    print(f"  Kept {len(keep_subset_ids)}/{len(subset_stats)} subsets")
    print(f"  Samples: {len(curated_df)} (was {len(df)})")
    print(f"  Reduction: {100 * (1 - len(curated_df)/len(df)):.1f}%")

    # Show kept subset IDs
    kept_ids = sorted(list(keep_subset_ids))
    print(f"  Kept subsets: {', '.join(kept_ids[:20])}" +
          (f" ... and {len(kept_ids)-20} more" if len(kept_ids) > 20 else ""))

    return curated_df

# Drive integration utilities
def setup_drive_connection():
    """Setup Google Drive connection for Colab"""
    try:
        from google.colab import drive
        print("[Drive] Mounting Google Drive...")
        drive.mount('/content/drive', force_remount=True)

        # Create results directory
        results_dir = "/content/drive/MyDrive/AGRIS_NLP_Results"
        os.makedirs(results_dir, exist_ok=True)
        print(f"[Drive] Results directory: {results_dir}")
        return results_dir
    except ImportError:
        print("[Drive] Not in Colab environment; using local storage")
        results_dir = "/content/results"
        os.makedirs(results_dir, exist_ok=True)
        return results_dir
    except Exception as e:
        print(f"[Drive] Error setting up drive: {e}")
        results_dir = "/content/results"
        os.makedirs(results_dir, exist_ok=True)
        return results_dir

def save_results_to_drive(results_df: pd.DataFrame, dataset_info: str, results_dir: str):
    """Save results to Google Drive with timestamping"""
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    base_name = f"agris_enhanced_eval_{timestamp}"

    try:
        # Save main results
        results_path = os.path.join(results_dir, f"{base_name}_results.csv")
        results_df.to_csv(results_path, index=False)
        print(f"[Drive] Saved results: {results_path}")

        # Save summary report
        summary_path = os.path.join(results_dir, f"{base_name}_summary.txt")
        with open(summary_path, 'w') as f:
            f.write(f"AGRIS Enhanced NLP Evaluation Results\n")
            f.write(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Dataset: {dataset_info}\n\n")

            # Filter and sort results
            eval_results = results_df[results_df["component"] != "generator"].copy()
            if len(eval_results) > 0:
                eval_results = eval_results.sort_values("acc_semantic", ascending=False)

                f.write("RANKING BY SEMANTIC ACCURACY:\n")
                f.write("=" * 50 + "\n")

                for idx, row in eval_results.iterrows():
                    f.write(f"\nRank {idx+1}: {row.get('model', 'Unknown')}\n")
                    f.write(f"  Component: {row.get('component', 'N/A')}\n")
                    f.write(f"  Semantic@1: {row.get('acc_semantic', 0):.1%}\n")
                    f.write(f"  Semantic@3: {row.get('acc_top3_semantic', 0):.1%}\n")
                    f.write(f"  Semantic@5: {row.get('acc_top5_semantic', 0):.1%}\n")
                    f.write(f"  Domain-weighted@1: {row.get('acc_domain_weighted', 0):.1%}\n")
                    f.write(f"  Fuzzy@1: {row.get('acc_fuzzy', 0):.1%}\n")
                    f.write(f"  Latency (p50): {row.get('latency_ms_p50', 0):.1f} ms\n")
                    f.write(f"  Samples: {row.get('n', 0)}\n")

                # Best model summary
                if len(eval_results) > 0:
                    best = eval_results.iloc[0]
                    f.write(f"\n\nBEST MODEL SUMMARY:\n")
                    f.write("=" * 30 + "\n")
                    f.write(f"Model: {best.get('model', 'Unknown')}\n")
                    f.write(f"Type: {best.get('component', 'N/A')}\n")
                    f.write(f"Semantic@1: {best.get('acc_semantic', 0):.1%}\n")
                    f.write(f"Domain-weighted@1: {best.get('acc_domain_weighted', 0):.1%}\n")

        print(f"[Drive] Saved summary: {summary_path}")

        return results_path, summary_path

    except Exception as e:
        print(f"[Drive] Error saving results: {e}")
        # Fallback to local save
        local_path = f"/content/{base_name}_results.csv"
        results_df.to_csv(local_path, index=False)
        print(f"[Drive] Fallback save: {local_path}")
        return local_path, None

# Enhanced Colab entrypoint
def run_enhanced_colab_evaluation():
    """Enhanced Colab evaluation with comprehensive improvements"""

    class EnhancedTestArgs:
        def __init__(self):
            # Dataset configuration
            self.data_csv = "/content/enhanced_agris_eval.csv"

            # Model configuration - enhanced selection
            self.extractor_models = (
                "recobo/agriculture-bert-uncased,"
                "allenai/scibert_scivocab_uncased,"
                "allenai/scibert_scivocab_cased,"
                "distilbert-base-uncased,"
                "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
            )

            # Enhanced ensemble pairs
            self.ensemble_pairs = [
                ("recobo/agriculture-bert-uncased", "allenai/scibert_scivocab_uncased"),
                ("recobo/agriculture-bert-uncased", "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext")
            ]

            # Technical parameters
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.topk = 15 
            self.robustness_noise_p = 0.06  

            # Data collection parameters
            self.build_csv_from_agris = True
            self.agris_catalog_xml = "/content/drive/MyDrive/AGRIS.ODS.xml"
            self.n_subsets = 40  
            self.max_records = 12000  

            # Enhanced filtering
            self.lang_filter = "en"
            self.keyword_filter = (
                # Core crops
                "maize,wheat,rice,corn,soybean,soya,cassava,sorghum,barley,oat,rye,millet,quinoa,"
                "cotton,coffee,cocoa,tea,sugarcane,potato,tomato,bean,pea,lentil,chickpea,"
                # Agricultural inputs
                "fertilizer,fertiliser,manure,compost,nitrogen,phosphorus,potassium,micronutrient,"
                "irrigation,water,soil,clay,loam,sand,organic,humus,ph,salinity,drainage,"
                # Pest management
                "pesticide,herbicide,insecticide,fungicide,pest,disease,weed,aphid,nematode,"
                "blight,rust,mildew,virus,bacteria,fungus,pathogen,resistance,tolerance,"
                # Climate and environment
                "drought,flood,temperature,rainfall,humidity,climate,weather,season,phenology,"
                "photosynthesis,transpiration,vernalization,photoperiod,"
                # Breeding and genetics
                "variety,cultivar,hybrid,breeding,genetic,gene,chromosome,allele,qtl,marker,"
                "transgenic,biotechnology,tissue,culture,mutation,selection,"
                # Measurements
                "yield,biomass,protein,starch,oil,fiber,quality,nutrition,hectare,kilogram,"
                "concentration,density,moisture,harvest,germination,emergence"
            )
            self.subset_filter = ""

            # Enhanced curation parameters
            self.do_subset_curation = True
            self.curation_keep_frac = 0.75  
            self.curation_diag_cap = 250    
            self.min_samples_per_subset = 25

    args = EnhancedTestArgs()

    print(" ENHANCED AGRIS NLP EVALUATION")
    print("=" * 60)
    print(f" Dataset target: {args.max_records} samples from {args.n_subsets} subsets")
    print(f" Models: {len(args.extractor_models.split(','))} individual + {len(args.ensemble_pairs)} ensembles")
    print(f" Device: {args.device}")
    print(f" Enhanced features: Domain weighting, morphological matching, fuzzy evaluation")

    # Setup Google Drive
    results_dir = setup_drive_connection()

    # Enhanced CSV builder 
    if args.build_csv_from_agris:
        try:
            print(f"\n Resolving AGRIS catalog...")
            args.agris_catalog_xml = ensure_catalog_xml(args.agris_catalog_xml)
            print(f" Using catalog: {args.agris_catalog_xml}")
        except Exception as e:
            print(f" Error finding catalog: {e}")
            return

        # Set global filters
        global LANG_FILTER, KEYWORD_FILTER
        LANG_FILTER = args.lang_filter or "en"
        KEYWORD_FILTER = [k.strip() for k in (args.keyword_filter or "").split(",") if k.strip()]
        print(f" Language filter: {LANG_FILTER}")
        print(f"  Keywords: {len(KEYWORD_FILTER)} terms")

        # Extract subset URLs
        sub_filters = [s.strip().upper() for s in (args.subset_filter or "").split(",") if s.strip()]
        subset_urls = extract_subset_urls(args.agris_catalog_xml, sub_filters if sub_filters else None)

        if not subset_urls:
            print(" No subset URLs found")
            return

        # Enhanced data collection
        headers = {"User-Agent": "Mozilla/5.0 (Enhanced-AGRIS-Eval/2.0)"}
        rows_by_subset: Dict[str, list] = {}
        selected_subsets = subset_urls[:args.n_subsets]

        print(f"\n⬇  Downloading {len(selected_subsets)} subsets...")
        download_success = 0

        for subset_id, title, url in tqdm(selected_subsets, desc=" Collecting subsets"):
            try:
                r = requests.get(url, headers=headers, timeout=120)
                r.raise_for_status()

                rows = parse_subset_xml_bytes(r.content, subset_id=subset_id)
                if rows:
                    rows_by_subset[subset_id] = rows
                    download_success += 1
                    print(f"   {subset_id}: {len(rows)} quality samples")
                else:
                    print(f"    {subset_id}: No valid samples")

            except Exception as e:
                print(f"   {subset_id}: {e}")

        print(f"\n Download summary: {download_success}/{len(selected_subsets)} successful")

        # Enhanced balanced sampling
        if rows_by_subset:
            print(f" Creating balanced dataset...")

            # Shuffle each subset
            rng = random.Random(42)
            for subset_id in rows_by_subset:
                rng.shuffle(rows_by_subset[subset_id])

            # Round-robin sampling with quality preference
            balanced_rows = []
            subsets = list(rows_by_subset.keys())
            subset_indices = {subset_id: 0 for subset_id in subsets}
            target_size = args.max_records

            round_count = 0
            while len(balanced_rows) < target_size and round_count < 1000:  
                progress_made = False

                for subset_id in subsets:
                    if len(balanced_rows) >= target_size:
                        break

                    rows = rows_by_subset[subset_id]
                    idx = subset_indices[subset_id]

                    if idx < len(rows):
                        balanced_rows.append(rows[idx])
                        subset_indices[subset_id] += 1
                        progress_made = True

                if not progress_made:
                    break

                round_count += 1

            # Save to CSV
            df = pd.DataFrame(balanced_rows)
            df.to_csv(args.data_csv, index=False)

            print(f" Created balanced dataset: {len(balanced_rows)} samples")
            print(f" Saved to: {args.data_csv}")

            # Dataset statistics
            if len(balanced_rows) > 0:
                subset_counts = df["subset_id"].value_counts()
                print(f" Subsets represented: {len(subset_counts)}")
                print(f" Samples per subset (avg): {subset_counts.mean():.1f}")
                print(f" Most common subsets: {list(subset_counts.head(5).index)}")
        else:
            print(" No data collected; evaluation will be skipped.")
            return

    # Load and curate data
    if not os.path.exists(args.data_csv) or os.path.getsize(args.data_csv) == 0:
        print(f" Dataset file missing or empty: {args.data_csv}")
        return

    print(f"\n Loading dataset from {args.data_csv}")
    df = pd.read_csv(args.data_csv)
    print(f" Loaded {len(df)} samples")

    # Show dataset composition
    if "subset_id" in df.columns:
        subset_counts = df["subset_id"].value_counts()
        print(f" Subset distribution (top 10):")
        for subset_id, count in subset_counts.head(10).items():
            print(f"   {subset_id}: {count} samples")

    # Enhanced curation
    if args.do_subset_curation and "subset_id" in df.columns:
        print(f"\n Starting enhanced subset curation...")
        df_curated = enhanced_curate_by_subset(
            df,
            device=args.device,
            topk=args.topk,
            diag_cap=args.curation_diag_cap,
            keep_frac=args.curation_keep_frac,
            min_samples_per_subset=args.min_samples_per_subset
        )

        if len(df_curated) > 0:
            df = df_curated
            print(f" Curation complete: {len(df)} samples retained")
        else:
            print("  Curation produced empty dataset; using original")

   
    print(f"\n ENHANCED MODEL EVALUATION")
    print("=" * 50)

    results = []

    # Individual models
    model_list = [m.strip() for m in args.extractor_models.split(",") if m.strip()]
    print(f" Evaluating {len(model_list)} individual models...")

    for i, model in enumerate(model_list):
        print(f"\n{'' * 60}")
        print(f" Model {i+1}/{len(model_list)}: {model}")
        print(f"{'' * 60}")

        try:
            result = eval_enhanced_extractor(
                df, model, args.device, args.topk, args.robustness_noise_p
            )
            result.update({"component": "extractor", "model": model})
            results.append(result)

            # Progress summary
            print(f" Completed {model.split('/')[-1]}")
            if "acc_semantic" in result:
                print(f"    Semantic@1: {result['acc_semantic']:.1%}")
                print(f"    Domain-weighted@1: {result.get('acc_domain_weighted', 0):.1%}")

        except Exception as e:
            print(f" Error evaluating {model}: {e}")
            continue

    # Enhanced ensemble evaluation
    print(f"\n ENHANCED ENSEMBLE EVALUATION")
    print("=" * 50)

    for i, (model_a, model_b) in enumerate(args.ensemble_pairs):
        print(f"\n{'' * 60}")
        print(f" Ensemble {i+1}/{len(args.ensemble_pairs)}: {model_a.split('/')[-1]} + {model_b.split('/')[-1]}")
        print(f"{'' * 60}")

        try:
            result = eval_enhanced_ensemble(
                df, model_a, model_b, args.device, args.topk, args.robustness_noise_p
            )
            result.update({
                "component": "ensemble",
                "model": f"{model_a.split('/')[-1]} + {model_b.split('/')[-1]}"
            })
            results.append(result)

            print(f" Completed ensemble {i+1}")
            if "acc_semantic" in result:
                print(f"    Semantic@1: {result['acc_semantic']:.1%}")
                print(f"    Domain-weighted@1: {result.get('acc_domain_weighted', 0):.1%}")

        except Exception as e:
            print(f" Error evaluating ensemble: {e}")
            continue

    #  Results Analysis 
    if not results:
        print(" No evaluation results available")
        return

    print(f"\n ENHANCED EVALUATION RESULTS")
    print("=" * 80)

    results_df = pd.DataFrame(results)

    # Display configuration
    pd.set_option("display.max_columns", None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 30)

    # Full results table
    print("\n COMPLETE RESULTS TABLE:")
    print("-" * 80)
    try:
        display_cols = [
            "component", "model", "acc_exact", "acc_semantic", "acc_fuzzy",
            "acc_domain_weighted", "acc_top5_semantic", "acc_top10_semantic",
            "latency_ms_p50", "n"
        ]
        available_cols = [col for col in display_cols if col in results_df.columns]
        print(results_df[available_cols].to_string(index=False, float_format="%.4f"))
    except Exception as e:
        print(f"Error displaying full results: {e}")
        print(results_df.to_string(index=False))

    # Enhanced ranking analysis
    try:
        eval_results = results_df[results_df["component"] != "generator"].copy()
        if len(eval_results) > 0:
            eval_results = eval_results.sort_values("acc_semantic", ascending=False)

            print(f"\n RANKING BY SEMANTIC@1 ACCURACY")
            print("=" * 80)

            ranking_cols = [
                "component", "model", "acc_semantic", "acc_domain_weighted",
                "acc_fuzzy", "acc_top3_semantic", "acc_top5_semantic",
                "acc_top10_semantic", "latency_ms_p50", "n"
            ]
            available_ranking_cols = [col for col in ranking_cols if col in eval_results.columns]

            print(eval_results[available_ranking_cols].to_string(index=False, float_format="%.4f"))

            # Winner analysis
            if len(eval_results) > 0:
                winner = eval_results.iloc[0]
                print(f"\n CHAMPION MODEL")
                print("=" * 40)
                print(f" Model: {winner['model']}")
                print(f" Type: {winner['component']}")
                print(f" Semantic@1: {winner.get('acc_semantic', 0):.1%}")
                print(f" Domain-weighted@1: {winner.get('acc_domain_weighted', 0):.1%}")
                print(f" Fuzzy@1: {winner.get('acc_fuzzy', 0):.1%}")
                print(f" Semantic@3: {winner.get('acc_top3_semantic', 0):.1%}")
                print(f" Semantic@5: {winner.get('acc_top5_semantic', 0):.1%}")
                print(f" Semantic@10: {winner.get('acc_top10_semantic', 0):.1%}")
                print(f" Latency (p50): {winner.get('latency_ms_p50', 0):.1f} ms")
                print(f" Evaluated on: {winner.get('n', 0)} samples")

                # Performance improvement analysis
                baseline_results = eval_results[eval_results["model"].str.contains("distilbert", case=False)]
                if len(baseline_results) > 0:
                    baseline = baseline_results.iloc[0]
                    if baseline.get('acc_semantic', 0) > 0:
                        improvement = (winner.get('acc_semantic', 0) - baseline.get('acc_semantic', 0)) / baseline.get('acc_semantic', 0) * 100
                        print(f" Improvement over DistilBERT: +{improvement:.1f}%")

    except Exception as e:
        print(f" Error in results analysis: {e}")

    #  Save Enhanced Results to Drive 
    print(f"\n SAVING RESULTS TO GOOGLE DRIVE")
    print("=" * 50)

    try:
        dataset_info = f"{len(df)} samples, {args.n_subsets} subsets, {len(model_list)} models + {len(args.ensemble_pairs)} ensembles"

        results_path, summary_path = save_results_to_drive(results_df, dataset_info, results_dir)

        print(f" Results saved successfully!")
        print(f" Main results: {results_path}")
        if summary_path:
            print(f" Summary report: {summary_path}")

        # Also save the dataset for reference
        try:
            dataset_backup = os.path.join(results_dir, f"dataset_backup_{time.strftime('%Y%m%d_%H%M%S')}.csv")
            df.to_csv(dataset_backup, index=False)
            print(f" Dataset backup: {dataset_backup}")
        except Exception as e:
            print(f"  Could not save dataset backup: {e}")

    except Exception as e:
        print(f" Error saving to drive: {e}")
        # Fallback local save
        local_results = "/content/enhanced_agris_results.csv"
        results_df.to_csv(local_results, index=False)
        print(f" Fallback save: {local_results}")

    #  Final Summary 
    print(f"\n ENHANCED EVALUATION COMPLETED SUCCESSFULLY!")
    print("=" * 80)
    print(f" Dataset: {len(df)} high-quality samples")
    print(f" Models: {len(results)} evaluated (individual + ensembles)")
    print(f" Device: {args.device}")
    print(f" Enhanced features:")
    print(f"   • Domain-aware scoring and weighting")
    print(f"   • Advanced morphological matching")
    print(f"   • Fuzzy string similarity")
    print(f"   • Contextual masking strategy")
    print(f"   • Enhanced ensemble fusion")
    print(f"   • Intelligent text truncation")
    print(f"   • Subset quality curation")

    if len(results) > 0:
        best_semantic = max(results, key=lambda x: x.get('acc_semantic', 0))
        print(f" Best semantic accuracy: {best_semantic.get('acc_semantic', 0):.1%} ({best_semantic.get('model', 'Unknown')})")

        best_domain = max(results, key=lambda x: x.get('acc_domain_weighted', 0))
        print(f" Best domain-weighted: {best_domain.get('acc_domain_weighted', 0):.1%} ({best_domain.get('model', 'Unknown')})")

    print(f"\n All results saved to Google Drive: {results_dir}")
    print(f" Access your results in Google Drive under 'AGRIS_NLP_Results' folder")

    return results_df

# CLI compatibility 
def main():
    """Enhanced CLI entry point"""
    import sys
    filtered_args = [arg for arg in sys.argv
                    if not any(pattern in arg.lower() for pattern in ['-f', 'kernel', 'json', 'ipython'])]

    parser = argparse.ArgumentParser(
        description="Enhanced AGRIS MLM Evaluation with Domain Knowledge",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Core arguments
    parser.add_argument("--data_csv", type=str, default="/content/enhanced_agris_eval.csv",
                       help="Path to evaluation dataset CSV")
    parser.add_argument("--models", type=str,
                       default="recobo/agriculture-bert-uncased,allenai/scibert_scivocab_uncased",
                       help="Comma-separated list of models to evaluate")
    parser.add_argument("--device", type=str, default="auto",
                       help="Device to use (cuda/cpu/auto)")
    parser.add_argument("--max_samples", type=int, default=10000,
                       help="Maximum number of samples to use")

    # Enhanced features
    parser.add_argument("--enable_curation", action="store_true", default=True,
                       help="Enable subset quality curation")
    parser.add_argument("--enable_ensemble", action="store_true", default=True,
                       help="Enable ensemble evaluation")
    parser.add_argument("--results_dir", type=str, default="/content/drive/MyDrive/AGRIS_NLP_Results",
                       help="Directory to save results")

    try:
        args = parser.parse_args(filtered_args[1:] if len(filtered_args) > 1 else [])
        print("Enhanced AGRIS Evaluation - CLI Mode")
        print("For full Colab experience, use: run_enhanced_colab_evaluation()")
    except:
        print("Enhanced AGRIS Evaluation")
        print("For Colab: run_enhanced_colab_evaluation()")
        print("For CLI: use main() with proper arguments")

if __name__ == "__main__":
    print(" Enhanced AGRIS NLP Evaluation System")
    print("=" * 50)
    print(" For Colab: run_enhanced_colab_evaluation()")
    print("  For CLI: main() with arguments")
    print(" Features: Domain knowledge, ensemble fusion, enhanced accuracy metrics")

    # Auto-run enhanced evaluation in Colab
    try:
        import google.colab
        print("\n Detected Colab environment - starting enhanced evaluation...")
        run_enhanced_colab_evaluation()
    except ImportError:
        print("\n CLI environment detected - use main() function")

from transformers import AutoTokenizer, AutoModelForMaskedLM

# Example: AgricultureBERT after fine-tuning
model_name = "recobo/agriculture-bert-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# After training/fine-tuning:
save_dir = "/content/artifacts/agri_bert"

model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print(f"Model + tokenizer saved to {save_dir}")

